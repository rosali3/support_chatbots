{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Токенизаторы\n",
    "Набиуллина Розалия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выводы (перед выполнением, чтобы сразу было видно)\n",
    "\n",
    "**nltk.sent_tokenize** - делит на большее количество предложений чем PunktSentenceTokenizer. Примерно одинаковы по скорости\n",
    "\n",
    "**PunktSentenceTokenizer** - логично судя по названию разбивает на предложения. Берет текст\n",
    "\n",
    "**nltk.word_tokenize** - медленно, не отделяет знаки препинания от слов\n",
    "\n",
    "**TreebankWordTokenizer** - работает хорошо, отделяет запятые от слов. Быстро\n",
    "\n",
    "**WordPunctTokenizer** - в пять раз быстрее TreebankWordTokenizer. также хорошо отделяет от знаков препинания\n",
    "\n",
    "**MWETokenizer** - разделяет по символам \n",
    "\n",
    "**мой токенайзер** - чудеснейшая вещь не понимаю почему другие токенайзеры не такие хорошие. И быстрый и без знаков препинания -- очем еще может мечтать маленький нлп специалист"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import SyllableTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "разобрать по предложениям выбрать с 100 по 115"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разбиваем на предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"I know what I mean, and you needn\\'t be _statirical_ about it.',\n",
       " 'It\\'s\\nproper to use good words, and improve your _vocabilary_,\" returned Amy,\\nwith dignity.',\n",
       " '\"Don\\'t peck at one another, children.',\n",
       " \"Don't you wish we had the money\\npapa lost when we were little, Jo?\",\n",
       " 'Dear me!',\n",
       " 'how happy and good we\\'d be,\\nif we had no worries!\"',\n",
       " 'said Meg, who could remember better times.',\n",
       " '\"You said the other day, you thought we were a deal happier than the\\nKing children, for they were fighting and fretting all the time, in\\nspite of their money.\"',\n",
       " '\"So I did, Beth.',\n",
       " 'Well, I think we are; for, though we do have to work,\\nwe make fun for ourselves, and are a pretty jolly set, as Jo would say.\"',\n",
       " '\"Jo does use such slang words!\"',\n",
       " 'observed Amy, with a reproving look at\\nthe long figure stretched on the rug.',\n",
       " 'Jo immediately sat up, put her\\nhands in her pockets, and began to whistle.',\n",
       " '\"Don\\'t, Jo; it\\'s so boyish!\"',\n",
       " '\"That\\'s why I do it.\"',\n",
       " '\"I detest rude, unlady-like girls!\"']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('f.txt', encoding = 'cp1252')\n",
    "file = f.read() # открываем файл\n",
    "sentences = sent_tokenize(file) # делим текст на предложения\n",
    "sentences[100:116]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"But I don\\'t think the little we should spend would do any good.',\n",
       " \"We've\\neach got a dollar, and the army wouldn't be much helped by our giving\\nthat.\",\n",
       " 'I agree not to expect anything from mother or you, but I do want\\nto buy Undine and Sintram for myself; I\\'ve wanted it _so_ long,\" said\\nJo, who was a bookworm.',\n",
       " '\"I planned to spend mine in new music,\" said Beth, with a little sigh,\\nwhich no one heard but the hearth-brush and kettle-holder.',\n",
       " '\"I shall get a nice box of Faber\\'s drawing-pencils; I really need them,\"\\nsaid Amy decidedly.',\n",
       " '\"Mother didn\\'t say anything about our money, and she won\\'t wish us to\\ngive up everything.',\n",
       " 'Let\\'s each buy what we want, and have a little fun;\\nI\\'m sure we work hard enough to earn it,\" cried Jo, examining the heels\\nof her shoes in a gentlemanly manner.',\n",
       " '\"I know _I_ do,--teaching those tiresome children nearly all day, when\\nI\\'m longing to enjoy myself at home,\" began Meg, in the complaining tone\\nagain.',\n",
       " '\"You don\\'t have half such a hard time as I do,\" said Jo.',\n",
       " '\"How would you\\nlike to be shut up for hours with a nervous, fussy old lady, who keeps\\nyou trotting, is never satisfied, and worries you till you\\'re ready to\\nfly out of the window or cry?\"',\n",
       " '\"It\\'s naughty to fret; but I do think washing dishes and keeping things\\ntidy is the worst work in the world.',\n",
       " 'It makes me cross; and my hands get\\nso stiff, I can\\'t practise well at all;\" and Beth looked at her rough\\nhands with a sigh that any one could hear that time.',\n",
       " '\"I don\\'t believe any of you suffer as I do,\" cried Amy; \"for you don\\'t\\nhave to go to school with impertinent girls, who plague you if you don\\'t\\nknow your lessons, and laugh at your dresses, and label your father if\\nhe isn\\'t rich, and insult you when your nose isn\\'t nice.\"',\n",
       " '\"If you mean _libel_, I\\'d say so, and not talk about _labels_, as if\\npapa was a pickle-bottle,\" advised Jo, laughing.',\n",
       " '\"I know what I mean, and you needn\\'t be _statirical_ about it.',\n",
       " 'It\\'s\\nproper to use good words, and improve your _vocabilary_,\" returned Amy,\\nwith dignity.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PunktSentenceTokenizer().tokenize(file) [100:116] # не работает с разбитыми на предложения текстом "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разбиваем на слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nltk.word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokens = nltk.word_tokenize(file.lower()) ##\n",
    "tokens[100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', 'Beth', ',', 'and', 'Amy', 'BY', 'LOUISA', 'M.', 'ALCOTT', 'AUTHOR']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TreebankWordTokenizer().tokenize(file) [100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 219 ms\n",
      "Wall time: 224 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['OR', 'Meg', ',', 'Jo', ',', 'Beth', ',', 'and', 'Amy', 'BY']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "WordPunctTokenizer().tokenize(file) [100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разбиваем на слоги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "SyllableTokenizer().tokenize(file) [100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разбиваем на символы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 906 ms\n",
      "Wall time: 1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' ', 'T', '.', ' ', 'M', 'e', 'r', 'r', 'i', 'l']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from nltk.tokenize import MWETokenizer\n",
    "MWETokenizer().tokenize(file) [100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Розаличкин токенайзер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jo\n",
      "laid\n",
      "her\n",
      "head\n",
      "on\n",
      "a\n",
      "comfortable\n",
      "rag-bag\n",
      "and\n",
      "cried\n",
      "524\n",
      "A\n",
      "substantial\n",
      "lifelike\n",
      "ghost\n",
      "leaning\n",
      "over\n",
      "her\n",
      "525\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "data = file.split()\n",
    "for i in data[1700:1720]:\n",
    "   print (i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e266e904f44d163170d6b96f3c5a30b92d4c50da2490bcfb0e99115032d4938"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
